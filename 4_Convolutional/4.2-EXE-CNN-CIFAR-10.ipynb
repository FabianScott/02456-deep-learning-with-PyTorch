{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oZW0gaQO81Sq"
   },
   "source": [
    "# CNN on CIFAR-10\n",
    "\n",
    "In this notebook you need to put what you have learned into practice, and create your own convolutional classifier for the CIFAR-10 dataset.\n",
    "\n",
    "The images in CIFAR-10 are RGB images (3 channels) with size 32x32 (so they have size 3x32x32). There are 10 different classes. See examples below.\n",
    "\n",
    "![cifar10](https://github.com/DeepLearningDTU/02456-deep-learning-with-PyTorch/blob/master/static_files/cifar10.png?raw=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "htyg7xxN81St"
   },
   "source": [
    "## Preliminaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 3009,
     "status": "ok",
     "timestamp": 1662640064042,
     "user": {
      "displayName": "Ole Winther",
      "userId": "12097569893493878263"
     },
     "user_tz": -120
    },
    "id": "v3u2GIWr81Su"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.utils import make_grid\n",
    "from sklearn import metrics\n",
    "from math import floor\n",
    "\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "def accuracy(target, pred):\n",
    "    return metrics.accuracy_score(target.detach().cpu().numpy(), pred.detach().cpu().numpy())\n",
    "\n",
    "def compute_confusion_matrix(target, pred, normalize=None):\n",
    "    return metrics.confusion_matrix(\n",
    "        target.detach().cpu().numpy(), \n",
    "        pred.detach().cpu().numpy(),\n",
    "        normalize=normalize\n",
    "    )\n",
    "\n",
    "def show_image(img):\n",
    "    img = img.detach().cpu()\n",
    "    img = img / 2 + 0.5   # unnormalize\n",
    "    with sns.axes_style(\"white\"):\n",
    "        plt.figure(figsize=(8, 8))\n",
    "        plt.imshow(img.permute((1, 2, 0)).numpy())\n",
    "        plt.axis('off')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 108,
     "referenced_widgets": [
      "2435970902984fb09c0a257131d742be",
      "548c47f6e8fa4a37a939535ef670020c",
      "9d6456ca327f4a858e92c13cd417340d",
      "67bbc7b98abf41f1927a7362aff86fe7",
      "b589c04a1dab4fd6a9c5cdd4c99d6cf9",
      "7289f6e93f2e44819780bb1b7918876b",
      "39e70cbbd65d4541a04c23c069b001ed",
      "0c6f066e4b44478a89c7384f6c284ece",
      "aa44e013a906441892f252576e269634",
      "07174f985c5b480b84ff77f61dd88f97",
      "30ae8a12c6a54ffcb37973b9ca01a07c"
     ]
    },
    "executionInfo": {
     "elapsed": 18641,
     "status": "ok",
     "timestamp": 1662640112137,
     "user": {
      "displayName": "Ole Winther",
      "userId": "12097569893493878263"
     },
     "user_tz": -120
    },
    "id": "QZeTujLC81S3",
    "outputId": "a75010e0-f99a-4ce5-eb6a-2df4f6150777"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# The output of torchvision datasets are PIL images in the range [0, 1]. \n",
    "# We transform them to PyTorch tensors and rescale them to be in the range [-1, 1].\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),  # subtract 0.5 and divide by 0.5\n",
    "    ]\n",
    ")\n",
    "\n",
    "batch_size = 64  # both for training and testing\n",
    "\n",
    "# Load datasets\n",
    "train_set = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "test_set = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=0, drop_last=False)\n",
    "test_loader = DataLoader(test_set, batch_size=batch_size, shuffle=False, num_workers=0, drop_last=True)\n",
    "\n",
    "# Map from class index to class name.\n",
    "classes = {index: name for name, index in train_set.class_to_idx.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 452,
     "status": "ok",
     "timestamp": 1662640116989,
     "user": {
      "displayName": "Ole Winther",
      "userId": "12097569893493878263"
     },
     "user_tz": -120
    },
    "id": "JDHkc52L81S9",
    "outputId": "2fe8610c-37c6-47c8-99e1-092299c6050f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data\n",
      "Number of points: 50000\n",
      "Batch dimension (B x C x H x W): torch.Size([64, 3, 32, 32])\n",
      "Number of distinct labels: 10 (unique labels: {0, 1, 2, 3, 4, 5, 6, 7, 8, 9})\n",
      "\n",
      "Test data\n",
      "Number of points: 10000\n",
      "Batch dimension (B x C x H x W): torch.Size([64, 3, 32, 32])\n",
      "Number of distinct labels: 10 (unique labels: {0, 1, 2, 3, 4, 5, 6, 7, 8, 9})\n"
     ]
    }
   ],
   "source": [
    "print(\"Training data\")\n",
    "print(\"Number of points:\", len(train_set))\n",
    "x, y = next(iter(train_loader))\n",
    "print(\"Batch dimension (B x C x H x W):\", x.shape)\n",
    "print(f\"Number of distinct labels: {len(set(train_set.targets))} (unique labels: {set(train_set.targets)})\")\n",
    "\n",
    "print(\"\\nTest data\")\n",
    "print(\"Number of points:\", len(test_set))\n",
    "x, y = next(iter(test_loader))\n",
    "print(\"Batch dimension (B x C x H x W):\", x.shape)\n",
    "print(f\"Number of distinct labels: {len(set(test_set.targets))} (unique labels: {set(test_set.targets)})\")\n",
    "\n",
    "n_classes = len(set(test_set.targets))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xSA1h94681TB"
   },
   "source": [
    "### Show example images\n",
    "\n",
    "Run multiple times to see different examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 466
    },
    "executionInfo": {
     "elapsed": 2498,
     "status": "ok",
     "timestamp": 1662640125654,
     "user": {
      "displayName": "Ole Winther",
      "userId": "12097569893493878263"
     },
     "user_tz": -120
    },
    "id": "njJy0klP81TD",
    "outputId": "decc1858-fd68-4969-8aca-b9194276967b"
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'_SingleProcessDataLoaderIter' object has no attribute 'next'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[4], line 2\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;66;03m# Get random training images and show them.\u001B[39;00m\n\u001B[1;32m----> 2\u001B[0m images, labels \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43miter\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mtrain_loader\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mnext\u001B[49m()\n\u001B[0;32m      3\u001B[0m show_image(torchvision\u001B[38;5;241m.\u001B[39mutils\u001B[38;5;241m.\u001B[39mmake_grid(images))\n",
      "\u001B[1;31mAttributeError\u001B[0m: '_SingleProcessDataLoaderIter' object has no attribute 'next'"
     ]
    }
   ],
   "source": [
    "# Get random training images and show them.\n",
    "images, labels = iter(train_loader).next()\n",
    "show_image(torchvision.utils.make_grid(images))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wt3BVFMF81TI"
   },
   "source": [
    "## Define a convolutional neural network\n",
    "\n",
    "\n",
    "**Assignment 1:** Define a convolutional neural network. \n",
    "You may use the code from previous notebooks.\n",
    "We suggest that you start with a small network, and make sure that everything is working.\n",
    "Once you can train successfully, come back and improve the architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 456,
     "status": "ok",
     "timestamp": 1662640362237,
     "user": {
      "displayName": "Ole Winther",
      "userId": "12097569893493878263"
     },
     "user_tz": -120
    },
    "id": "_EsKbw3o81TK",
    "outputId": "9ea47766-bca0-488e-8166-319efc4506b5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model(\n",
      "  (net): Sequential(\n",
      "    (0): Conv2d(3, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU()\n",
      "    (2): Conv2d(3, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (3): ReLU()\n",
      "    (4): Conv2d(3, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (5): ReLU()\n",
      "    (6): Conv2d(3, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (7): ReLU()\n",
      "    (8): Conv2d(3, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (9): ReLU()\n",
      "    (10): Flatten(start_dim=1, end_dim=-1)\n",
      "    (11): Linear(in_features=3072, out_features=1536, bias=True)\n",
      "    (12): ReLU()\n",
      "    (13): Linear(in_features=1536, out_features=768, bias=True)\n",
      "    (14): ReLU()\n",
      "    (15): Linear(in_features=768, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "channels = 3\n",
    "height, width = 32, 32\n",
    "class PrintSize(nn.Module):\n",
    "    \"\"\"Utility module to print current shape of a Tensor in Sequential, only at the first pass.\"\"\"\n",
    "    \n",
    "    first = True\n",
    "    \n",
    "    def forward(self, x):\n",
    "        if self.first:\n",
    "            print(f\"Size: {x.size()}\")\n",
    "            self.first = False\n",
    "        return x\n",
    "\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, num_classes, padding=1, kernelHeight=1, stride=1):\n",
    "        super().__init__()\n",
    "        self.first = True\n",
    "        self.num_classes = num_classes\n",
    "        activation_fn = nn.ReLU\n",
    "        outHeight = floor((height + padding*2 - (kernelHeight - 1) - 1)/stride) + 1\n",
    "        outWidth = floor((width + padding*2 - (kernelHeight - 1)  - 1)/stride) + 1\n",
    "        n_features = 3072\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=channels, out_channels=channels, kernel_size=1 + kernelHeight*2, stride=stride, padding=padding),\n",
    "            activation_fn(),\n",
    "            nn.Conv2d(in_channels=channels, out_channels=channels, kernel_size=1 + kernelHeight*2, stride=stride, padding=padding),\n",
    "            activation_fn(),\n",
    "            nn.Conv2d(in_channels=channels, out_channels=channels, kernel_size=1 + kernelHeight*2, stride=stride, padding=padding),\n",
    "            activation_fn(),\n",
    "            nn.Conv2d(in_channels=channels, out_channels=channels, kernel_size=1 + kernelHeight*2, stride=stride, padding=padding),\n",
    "            activation_fn(),\n",
    "            nn.Conv2d(in_channels=channels, out_channels=channels, kernel_size=1 + kernelHeight*2, stride=stride, padding=padding),\n",
    "            activation_fn(),\n",
    "\n",
    "            nn.Flatten(),  # from (1, channels, height, width) to (1, channels * height * width)\n",
    "            nn.Linear(n_features, n_features//2),\n",
    "            activation_fn(),\n",
    "            nn.Linear(n_features//2, n_features//4),\n",
    "            activation_fn(),\n",
    "            nn.Linear(n_features//4, num_classes),\n",
    "            # nn.Softmax(num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "model = Model(n_classes)\n",
    "device = torch.device('cpu')  # use cuda or cpu\n",
    "model.to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7-IUg3sq81TQ"
   },
   "source": [
    "## Define a loss function and optimizer\n",
    "\n",
    "**Assignment 2:** Define the loss function and optimizer.\n",
    "You might need to experiment a bit with the learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "executionInfo": {
     "elapsed": 447,
     "status": "ok",
     "timestamp": 1662640370072,
     "user": {
      "displayName": "Ole Winther",
      "userId": "12097569893493878263"
     },
     "user_tz": -120
    },
    "id": "48AX85QP81TR"
   },
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()  # Your code here!\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)  # Your code here!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-WneIN7C81TV"
   },
   "source": [
    "## Train the network\n",
    "\n",
    "**Assignment 3:** Finish the training loop below. \n",
    "Start by using a small number of epochs (e.g. 2).\n",
    "Even with a low number of epochs you should be able to see results that are better than chance.\n",
    "When everything is working increase the number of epochs to find out how good your network really is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "iWT0ULctWvm1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: torch.Size([2, 10])\n",
      "Output logits:\n",
      "[[ 0.02503814 -0.00994538  0.01162234 -0.03467641  0.01546746  0.03577084\n",
      "   0.02817579  0.02114907  0.01868353 -0.00244076]\n",
      " [ 0.02545453 -0.01000929  0.01090279 -0.03520943  0.01494167  0.03567851\n",
      "   0.02765018  0.02053994  0.01981567 -0.00411543]]\n",
      "Output probabilities:\n",
      "[[0.10140532 0.09791914 0.10005397 0.0955272  0.10043943 0.10249954\n",
      "  0.101724   0.10101172 0.10076298 0.09865676]\n",
      " [0.10147952 0.09794374 0.10001351 0.09550638 0.10041827 0.10252237\n",
      "  0.10170258 0.10098201 0.10090891 0.09852271]]\n"
     ]
    }
   ],
   "source": [
    "# Test the forward pass with dummy data\n",
    "out = model(torch.randn(2, 3, 32, 32, device=device))\n",
    "print(\"Output shape:\", out.size())\n",
    "print(f\"Output logits:\\n{out.detach().cpu().numpy()}\")\n",
    "print(f\"Output probabilities:\\n{out.softmax(1).detach().cpu().numpy()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NkUanRRb81TW",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 500     training accuracy: 0.181375\n",
      "             test accuracy: 0.2798\n",
      "Step 1000    training accuracy: 0.29995699541284404\n",
      "             test accuracy: 0.3127\n",
      "Step 1500    training accuracy: 0.32425\n",
      "             test accuracy: 0.3457\n",
      "Step 2000    training accuracy: 0.35238675458715596\n",
      "             test accuracy: 0.3697\n",
      "Step 2500    training accuracy: 0.37398538961038963\n",
      "             test accuracy: 0.3781\n",
      "Step 3000    training accuracy: 0.384875\n",
      "             test accuracy: 0.3916\n",
      "Step 3500    training accuracy: 0.39654737903225806\n",
      "             test accuracy: 0.4082\n",
      "Step 4000    training accuracy: 0.41597222222222224\n",
      "             test accuracy: 0.4162\n",
      "Step 4500    training accuracy: 0.4151875\n",
      "             test accuracy: 0.4158\n",
      "Step 5000    training accuracy: 0.4212155032467532\n",
      "             test accuracy: 0.4218\n",
      "Step 5500    training accuracy: 0.4483173076923077\n",
      "             test accuracy: 0.4318\n",
      "Step 6000    training accuracy: 0.4366875\n",
      "             test accuracy: 0.4362\n",
      "Step 6500    training accuracy: 0.44556864754098363\n",
      "             test accuracy: 0.4407\n",
      "Step 7000    training accuracy: 0.4435625\n",
      "             test accuracy: 0.4449\n",
      "Step 7500    training accuracy: 0.45133252164502163\n",
      "             test accuracy: 0.4497\n",
      "Step 8000    training accuracy: 0.4612847222222222\n",
      "             test accuracy: 0.4544\n",
      "Step 8500    training accuracy: 0.46340625\n",
      "             test accuracy: 0.4597\n",
      "Step 9000    training accuracy: 0.4711447864321608\n",
      "             test accuracy: 0.4586\n",
      "Step 9500    training accuracy: 0.4722521551724138\n",
      "             test accuracy: 0.4651\n",
      "Step 10000   training accuracy: 0.480125\n",
      "             test accuracy: 0.4695\n",
      "Step 10500   training accuracy: 0.4765625\n",
      "             test accuracy: 0.4729\n",
      "Step 11000   training accuracy: 0.50390625\n",
      "             test accuracy: 0.4697\n",
      "Step 11500   training accuracy: 0.49415625\n",
      "             test accuracy: 0.4805\n",
      "Step 12000   training accuracy: 0.5018518518518519\n",
      "             test accuracy: 0.4815\n",
      "Step 12500   training accuracy: 0.49809375\n",
      "             test accuracy: 0.4759\n",
      "Step 13000   training accuracy: 0.5070120389344263\n",
      "             test accuracy: 0.4865\n",
      "Step 13500   training accuracy: 0.5212378640776699\n",
      "             test accuracy: 0.4846\n",
      "Step 14000   training accuracy: 0.51084375\n",
      "             test accuracy: 0.4864\n",
      "Step 14500   training accuracy: 0.5188679245283019\n",
      "             test accuracy: 0.488\n",
      "Step 15000   training accuracy: 0.5315801056338029\n",
      "             test accuracy: 0.4933\n",
      "Step 15500   training accuracy: 0.52790625\n",
      "             test accuracy: 0.4957\n",
      "Step 16000   training accuracy: 0.5380642361111111\n",
      "             test accuracy: 0.497\n",
      "Step 16500   training accuracy: 0.5448717948717948\n",
      "             test accuracy: 0.4989\n",
      "Step 17000   training accuracy: 0.54378125\n",
      "             test accuracy: 0.4971\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "num_epochs = 100\n",
    "validation_every_steps = 500\n",
    "\n",
    "step = 0\n",
    "model.train()\n",
    "\n",
    "train_accuracies = []\n",
    "valid_accuracies = []\n",
    "from tqdm import tqdm\n",
    "for epoch in tqdm(range(num_epochs)):\n",
    "    \n",
    "    train_accuracies_batches = []\n",
    "    \n",
    "    for inputs, targets in train_loader:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        \n",
    "        # Forward pass, compute gradients, perform one training step.\n",
    "        # Your code here!\n",
    "        output = model.forward(inputs)\n",
    "        loss = loss_fn(output, targets)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # Increment step counter\n",
    "        step += 1\n",
    "        \n",
    "        # Compute accuracy.\n",
    "        predictions = output.max(1)[1]\n",
    "        train_accuracies_batches.append(accuracy(targets, predictions))\n",
    "        \n",
    "        if step % validation_every_steps == 0:\n",
    "            \n",
    "            # Append average training accuracy to list.\n",
    "            train_accuracies.append(np.mean(train_accuracies_batches))\n",
    "            \n",
    "            train_accuracies_batches = []\n",
    "        \n",
    "            # Compute accuracies on validation set.\n",
    "            valid_accuracies_batches = []\n",
    "            with torch.no_grad():\n",
    "                model.eval()\n",
    "                for inputs, targets in test_loader:\n",
    "                    inputs, targets = inputs.to(device), targets.to(device)\n",
    "                    output = model(inputs)\n",
    "                    loss = loss_fn(output, targets)\n",
    "\n",
    "                    predictions = output.max(1)[1]\n",
    "\n",
    "                    # Multiply by len(x) because the final batch of DataLoader may be smaller (drop_last=False).\n",
    "                    valid_accuracies_batches.append(accuracy(targets, predictions) * len(inputs))\n",
    "\n",
    "                model.train()\n",
    "                \n",
    "            # Append average validation accuracy to list.\n",
    "            valid_accuracies.append(np.sum(valid_accuracies_batches) / len(test_set))\n",
    "     \n",
    "            print(f\"Step {step:<5}   training accuracy: {train_accuracies[-1]}\")\n",
    "            print(f\"             test accuracy: {valid_accuracies[-1]}\")\n",
    "\n",
    "print(\"Finished training.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "targets[1], output[0]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0qAsbC8I81Ta"
   },
   "source": [
    "## Test the network\n",
    "\n",
    "Now we show a batch of test images and generate a table below with the true and predicted class for each of these images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7LT0RoAC81Tc"
   },
   "outputs": [],
   "source": [
    "inputs, targets = iter(test_loader).next()\n",
    "inputs, targets = inputs.to(device), targets.to(device)\n",
    "show_image(make_grid(inputs))\n",
    "plt.show()\n",
    "\n",
    "outputs = model(inputs)\n",
    "_, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "print(\"    TRUE        PREDICTED\")\n",
    "print(\"-----------------------------\")\n",
    "for target, pred in zip(targets, predicted):\n",
    "    print(f\"{classes[target.item()]:^13} {classes[pred.item()]:^13}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ISA6LJJO81Tg"
   },
   "source": [
    "We now evaluate the network as above, but on the entire test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Smv6_BwF81Ti"
   },
   "outputs": [],
   "source": [
    "# Evaluate test set\n",
    "confusion_matrix = np.zeros((n_classes, n_classes))\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    test_accuracies = []\n",
    "    for inputs, targets in test_loader:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        output = model(inputs)\n",
    "        loss = loss_fn(output, targets)\n",
    "\n",
    "        predictions = output.max(1)[1]\n",
    "\n",
    "        # Multiply by len(inputs) because the final batch of DataLoader may be smaller (drop_last=True).\n",
    "        test_accuracies.append(accuracy(targets, predictions) * len(inputs))\n",
    "        \n",
    "        confusion_matrix += compute_confusion_matrix(targets, predictions)\n",
    "\n",
    "    test_accuracy = np.sum(test_accuracies) / len(test_set)\n",
    "    \n",
    "    model.train()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ylxBAyXwI7Ab"
   },
   "source": [
    "Here we report the **average test accuracy** (number of correct predictions divided by test set size)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a5-c_CDAI7Ab"
   },
   "outputs": [],
   "source": [
    "print(f\"Test accuracy: {test_accuracy:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uMfUqY9rI7Ab"
   },
   "source": [
    "Here we look a bit more in depth into the performance of the classifier, using the **confusion matrix**. The entry at the i-th row and j-th column indicates the number of samples with true label being the i-th class and predicted label being the j-th class.\n",
    "\n",
    "We normalize the rows: given all examples of a specific class (row), we can observe here how they are classified by our model. Ideally, we would like the entries on the diagonals to be 1, and everything else 0. This would mean that all examples from that class are classified correctly.\n",
    "\n",
    "The classes that are harder to classify for our model have lower numbers on the diagonal. We can then see exactly *how* they are misclassified by looking at the rest of the row.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ShcEZDExI7Ab"
   },
   "outputs": [],
   "source": [
    "def normalize(matrix, axis):\n",
    "    axis = {'true': 1, 'pred': 0}[axis]\n",
    "    return matrix / matrix.sum(axis=axis, keepdims=True)\n",
    "\n",
    "x_labels = [classes[i] for i in classes]\n",
    "y_labels = x_labels\n",
    "plt.figure(figsize=(6, 6))\n",
    "sns.heatmap(\n",
    "    ax=plt.gca(),\n",
    "    data=normalize(confusion_matrix, 'true'),\n",
    "    annot=True,\n",
    "    linewidths=0.5,\n",
    "    cmap=\"Reds\",\n",
    "    cbar=False,\n",
    "    fmt=\".2f\",\n",
    "    xticklabels=x_labels,\n",
    "    yticklabels=y_labels,\n",
    ")\n",
    "plt.xticks(rotation=90)\n",
    "plt.yticks(rotation=0)\n",
    "plt.ylabel(\"True class\")\n",
    "plt.xlabel(\"Predicted class\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lw13Y86VI7Ac"
   },
   "source": [
    "Here we focus on the diagonal and plot the numbers in a bar plot. This gives us a clearer picture of the accuracy of the model for different classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xv75wMhVI7Ac"
   },
   "outputs": [],
   "source": [
    "with sns.axes_style('whitegrid'):\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    sns.barplot(x=x_labels, y=np.diag(normalize(confusion_matrix, 'true')))\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.title(\"Per-class accuracy\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ocnQOBAl81Tn"
   },
   "source": [
    "**Assignment 4:** \n",
    "1. Go back and improve performance of the network. By using enough convolutional layers with enough channels (and by training for long enough), you should easily be able to get a test accuracy above 60%, but see how much further you can get it! Can you reach 70%?\n",
    "\n",
    "2. Briefly describe what you did and any experiments you did along the way as well as what results you obtained.\n",
    "Did anything surprise you during the exercise?\n",
    "What were the changes that seemed to improve performance the most?\n",
    "\n",
    "3. Write down key lessons/insights you got during this exercise.\n",
    "\n",
    "**Answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8Nzefavy81To"
   },
   "source": [
    "# Training on GPU\n",
    "\n",
    "**Optional Assignment:**\n",
    "If you have a GPU, we suggest that you try training your model on GPU. For this, you need to move the model to GPU after defining it, which will recursively go over all modules and convert their parameters and buffers to CUDA tensors. You also need to transfer both the inputs and targets to GPU at each training step, before performing the forward pass.\n",
    "\n",
    "The code for this is already in place: notice the `.to(device)` statements. The only thing left to do is change the definition of `device` from `'cpu'` to `'cuda'`.\n",
    "\n",
    "If you don't have a GPU, you can do this on [Google Colab](https://research.google.com/colaboratory/).\n",
    "\n",
    "Use the code below to check if any GPU is avaiable in your current setup. This should print the models of all available GPUs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I09rtehbaCRH"
   },
   "outputs": [],
   "source": [
    "# Check if we have GPUs available\n",
    "print(\"Available CUDA devices:\", [torch.cuda.get_device_name(i) for i in range(torch.cuda.device_count())])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "actFGqQZM9Vd"
   },
   "source": [
    "You may not notice any significant speed-up from using a GPU. This is probably because your network is really small. Try increasing the width of your network (number of channels in the convolutional layers) and see if you observe any speed-up on GPU compared to CPU."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b8mEIylU81Tp"
   },
   "source": [
    "# Exercise from Michael Nielsen's book\n",
    "\n",
    "**Assignment 5:** Pick an exercise of your own choice from [Michael Nielsen's book](http://neuralnetworksanddeeplearning.com/).\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qh7KTAdWD_zx"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "07174f985c5b480b84ff77f61dd88f97": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "0c6f066e4b44478a89c7384f6c284ece": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2435970902984fb09c0a257131d742be": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_548c47f6e8fa4a37a939535ef670020c",
       "IPY_MODEL_9d6456ca327f4a858e92c13cd417340d",
       "IPY_MODEL_67bbc7b98abf41f1927a7362aff86fe7"
      ],
      "layout": "IPY_MODEL_b589c04a1dab4fd6a9c5cdd4c99d6cf9"
     }
    },
    "30ae8a12c6a54ffcb37973b9ca01a07c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "39e70cbbd65d4541a04c23c069b001ed": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "548c47f6e8fa4a37a939535ef670020c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_7289f6e93f2e44819780bb1b7918876b",
      "placeholder": "​",
      "style": "IPY_MODEL_39e70cbbd65d4541a04c23c069b001ed",
      "value": "100%"
     }
    },
    "67bbc7b98abf41f1927a7362aff86fe7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_07174f985c5b480b84ff77f61dd88f97",
      "placeholder": "​",
      "style": "IPY_MODEL_30ae8a12c6a54ffcb37973b9ca01a07c",
      "value": " 170498071/170498071 [00:13&lt;00:00, 13829808.62it/s]"
     }
    },
    "7289f6e93f2e44819780bb1b7918876b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9d6456ca327f4a858e92c13cd417340d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_0c6f066e4b44478a89c7384f6c284ece",
      "max": 170498071,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_aa44e013a906441892f252576e269634",
      "value": 170498071
     }
    },
    "aa44e013a906441892f252576e269634": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "b589c04a1dab4fd6a9c5cdd4c99d6cf9": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
